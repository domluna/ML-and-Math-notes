Conv Nets
---------

A conv layer typically consists of the 3 stages:

  1. convolutions (in parallel), affine transform
  2. nonlinear activation function (eg. ReLU)
  3. pooling function (detector stage)

### Pooling

An example of a pooling function is max pooling, there the function takes
the max activation value from the connections in the previous layer. The
functions themselves can have hyperparameters. For example in the max pooling
layer we described we can have a region size, which is essentially the number
of activation we max over.

We can think of this function as a summary statistic of the nearby outputs.

In all cases, pooling helps make the representation become invariant to small
translations of the input. This means if we translate the input by a small
amount most of the output values don't change. Useful for when we care more about
whether a feature is present vs. exactly where it is.

Data used with a conv net usually consists of several channels, each channel being
the observation of a different quantity at some point in space or time.

Conv operations can adapt to the size of the input, this isn't possible with normal
matrix multiplication layers. The kernel is applied a different number of times depending
on the size of the input.

### Data Examples

Data can of different dimensionality (1D, 2D, 3D). Both single channel and multi channel can occur.

### Connection to FFT

Convolution is equiv to converting both the input and the kernel to the frequency domain using
a Fourier transform, performing point-wise multiplication of the two signals and converting back
to the time domain using an inverse Fourier transform.

When a d-dimensional kernel can be expressed as the outer product of d vectors, one vector per dimension, the kernel is seperable. This results in O(w * d) runtime and storage vs O(w^d) from the naive convolution approach.

### Extra Notes

If we want to maintain a certain input/output size we can add zero-padding similar to crypto.
